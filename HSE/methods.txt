Метод стохастического градиента
Преимущества:
он легко реализуется, 
может быть обобщён на нелинейной модели. 
Он может быть использован с самыми разными функциями потерь, 
может быть использован для больших данных,

Недостатки. 
Во-первых, как правило, функционалы, которые получаются в задачах машинного обучения, являются многоэкстремальными, а какие-то обоснования сходимости для метода стохастического градиента известны главным образом только для выпуклых функций. А мы пытаемся его применять для невыпуклых функций, поэтому сходимость есть, но она только локальная, к локальному экстремуму, и как найти хорошее начальное приближение, которое приведёт нас к удачному локальному экстремуму, это важный вопрос. 
Дальше, возможна также расходимость или очень медленная сходимость, поэтому нужно знать, какими способами можно ускорить сходимость этого метода. 
Наконец, в линейных моделях возможно переобучение из-за неприятного эффекта, который называется мультиколлинеарностью



Чем хорош метод опорных векторов (SVM)? 
Тем, что задача построения линейного классификатора сводится к задаче выпуклого квадратичного программирования, которая имеет единственное решение. 
Имеются эффективные численные методы для решения этой задачи. 
Кроме того, полезным побочным результатом оказывается выделение множества опорных объектов, тех объектов, которые лежат вблизи границы классов.

 Недостатками метода является то, что 
 опорным объектом может стать также объект выброс, который имеет отрицательный отступ, лежит в толще объектов чужого класса, и на нем происходит ошибка. Но тем не менее, вот такие объекты тоже становятся опорными, и от них решение существенным образом зависит. Хотя, наверное, их надо было бы просто исключить из обучающей выборки. 
 Ну и некую сложность создает необходимость подбора константы C, впрочем, об этом мы поговорим в следующий раз. 